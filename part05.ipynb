{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Advanced Machine Learning Models\n",
    "<b>Author</b>: Sterling Cutler\n",
    "<br>\n",
    "<b>Date</b>: March 24, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting, Bagging, and Ensemble Methods\n",
    "Link: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df <- read.csv(\"ABI_data.csv\")\n",
    "\n",
    "# Train/test split\n",
    "train_ind <- sample(nrow(df), size=round(nrow(df)*0.8), replace=FALSE)\n",
    "x_train <- data.matrix(df[train_ind, 1:6])\n",
    "x_test <- data.matrix(df[-train_ind, 1:6])\n",
    "y_train <- df[train_ind, 7]\n",
    "y_test <- df[-train_ind, 7]\n",
    "\n",
    "# Print data shapes\n",
    "cat('Train Data Shape:', dim(x_train), \"\\n\")\n",
    "cat('Test Data Shape:', dim(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine (GBM)\n",
    "Doc: https://cran.r-project.org/web/packages/gbm/gbm.pdf\n",
    "\n",
    "Link: https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gbm)\n",
    "\n",
    "# Fit model to training data\n",
    "gbm <- gbm.fit(x_train, y_train, distribution=\"gaussian\", n.trees=1000,\n",
    "           shrinkage=0.5, bag.fraction=0.5)\n",
    "print(gbm)\n",
    "summary(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use out-of-bag estimator (OOB) to find optimal number of iterations\n",
    "best_iter <- gbm.perf(gbm, method=\"OOB\")\n",
    "print(best_iter)\n",
    "summary(gbm, n.trees=best_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value plots?\n",
    "par(mfrow=c(2,3))\n",
    "plot(gbm, 1, best.iter)\n",
    "plot(gbm, 2, best.iter)\n",
    "plot(gbm, 3, best.iter)\n",
    "plot(gbm, 4, best.iter)\n",
    "plot(gbm, 5, best.iter)\n",
    "plot(gbm, 6, best.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Metrics)\n",
    "\n",
    "# Predict target for test data\n",
    "gbm_preds <- predict(gbm, x_test, best_iter)\n",
    "paste(\"RMSE:\", round(rmse(y_test, gbm_preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Doc: https://cran.r-project.org/web/packages/randomForest/randomForest.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "\n",
    "# Fit model to training data\n",
    "rf <- randomForest(x_train, y_train, x_test, y_test, ntree=500)\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune model parameters with cross validation\n",
    "rf_cv <- rfcv(x_train, y_train, cv.fold=5)\n",
    "paste(rf_cv$n.var, rf_cv$error.cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "varImpPlot(rf, sort=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target for test data\n",
    "rf_preds <- predict(rf, x_test)\n",
    "paste(\"RMSE:\", round(rmse(y_test, rf_preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST\n",
    "Doc: https://cran.r-project.org/web/packages/xgboost/xgboost.pdf\n",
    "\n",
    "Link: http://xgboost.readthedocs.io/en/latest/model.html#objective-function-training-loss-regularization\n",
    "\n",
    "Versus GBM:\n",
    "- Adds regularization term that helps model avoid overfitting\n",
    "- Other hardware/computational benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "\n",
    "# Fit model to training data\n",
    "dtrain <- xgb.DMatrix(x_train, label=y_train)\n",
    "cntrl = list(eta=0.3, max_depth=6)\n",
    "xgb <- xgb.train(params=cntrl, data=x_train, nrounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune model parameters with cross validation\n",
    "cv <- xgb.cv(data=dtrain, nrounds=3, nfold=5, metrics=list(\"rmse\", \"auc\"), objective=\"reg:linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "# Show and plot feature importance\n",
    "xgb.importance(xgb)\n",
    "xgb.ggplot.importance(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target for test data\n",
    "xgb_preds <- xgb.predict(x_test)\n",
    "paste(\"RMSE:\", round(rmse(y_test, xgb_preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note: Distributed Modeling\n",
    "Sometimes datasets are too large to model on a single machine in a stable and timely manner.\n",
    "... talk about H20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
