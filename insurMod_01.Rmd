---
title: "Part 1: Exploratory Data Analysis"
author: Sterling Cutler
date: March 19, 2018
output: rmarkdown::github_document
---

## Introduction
Insurance companies use pricing (or rate making) models to determine premiums rates. A rate is an estimate of the expected value of a future cost. In insurance, rates are used to cover the expected cost of loss events. There are three primary regulations governing how insurers set these rates:

1.) they must be *fair* compared to the risk

2.) they must be *adequate* to maintain insurer solvency

3.) they are *not discriminatory* against members of an underwriting class with similar risk profiles

Another important consideration in designing models is the interpretability and underlying statistical basis on which it operates. A Casualty Actuarial and Statistical Task Force White Paper[1] states, "The Task Force recommends that rating plans in which insureds are grouped into homogeneous rating classes should not be so granular that resulting rating classes have little actuarial or statistical reliability. The use of sophisticated data analysis to develop finely tuned methodologies with a multiplicity of possible rating cells is not, in and of itself, a violation of rating laws as long as the rating classes and rating factors are cost-based."

Given the various constraints imposed on insurance modeling, generalized linear models are frequently used. This report will demonstrate examples of using linear models to interpret and predict auto bodily injury losses. The dataset is provided in the "insuranceData" package.

Description of the variables[3] are as follows: 

- CASENUM: case number to identify the claim

- ATTORNEY: whether the claimant is represented by an attorney (=1 if yes and =2 if no)

- CLMSEX: claimant's gender (=1 if male and =2 if female)

- MARITAL: claimant's marital status (=1 if married, =2 if single, =3 if widowed, and =4 if divorced/separated)

- CLMINSUR: whether or not the driver of the claimant's vehicle was uninsured (=1 if yes, =2 if no, and =3 if not applicable)

- SEATBELT: whether or not the claimant was wearing a seatbelt/child restraint (=1 if yes, =2 if no, and =3 if not applicable)

- CLMAGE: claimant's age

- LOSS The claimant's total economic loss (in thousands)

## Exploratory Data Analysis (EDA)
Before applying a model to a dataset, it's important to conduct exploratory data analysis to understand the distribution and relationships between the indepedent and dependent variables. Data Scientist Daniel Gutierrez explains[2], "The goal of exploratory data analysis is to obtain confidence in your data to a point where you're ready to engage a machine learning algorithm...In a hurry to get to the machine learning stage, some data scientists either entirely skip the exploratory process or do a very perfunctory job. This is a mistake with many implications, including generating inaccurate models, generating accurate models but on the wrong data, not creating the right types of variables in data preparation, and using resources inefficiently because of realizing only after generating models that perhaps the data is skewed, or has outliers, or has too many missing values, or finding that some values are inconsistent."

We'll start by examining the structure of the data including value types and distributions.

```{r struct}
# Load dataset
library(insuranceData)
data(AutoBi)

# Explore data
dim(AutoBi)
names(AutoBi)
str(AutoBi)
head(AutoBi)
```

Next, we'll inspect missing value counts. Because four of the five columns with missing values are categorical, and thereby coded as integers, we'll replace missing values with the column median.

```{r nulls}
# Count and replace nulls with column median
nulls <- sapply(AutoBi, function(col) sum(is.na(col)))
nulls
for(i in 1:ncol(AutoBi)){
  AutoBi[is.na(AutoBi[,i]), i] <- median(AutoBi[,i], na.rm = TRUE)
}
```

We'll normalize the claim age variable, remove an outlier ($1067.70) from the target variable and take the log distribution to keep it in scale with the other variables.

```{r normalize}
# Normalize Claim Age and show histograms
AutoBi$NORM_CLMAGE <- (AutoBi$CLMAGE-mean(AutoBi$CLMAGE))/sd(AutoBi$CLMAGE)
par(mfrow=c(1, 2))
hist(AutoBi$CLMAGE, main="Claim Age")
hist(AutoBi$NORM_CLMAGE, main="Normalized Claim Age")

# Inspect target variable and remove outlier
summary(AutoBi$LOSS)
sort(unique(AutoBi$LOSS), decreasing = T)[1:5]
AutoBi <- subset(AutoBi, LOSS < 300)

#Take log of target variable and show histograms
AutoBi$TARGET <- with(AutoBi, log(LOSS))
par(mfrow=c(2, 2))
hist(AutoBi$LOSS, main="Loss Histogram")
hist(AutoBi$TARGET, main="LOG(Loss) Histogram")

# Show Q-Q plots
qqnorm(AutoBi$LOSS, main="Loss Q-Q Plot")
qqline(AutoBi$LOSS, col='red')
qqnorm(AutoBi$TARGET, main="LOG(Loss) Q-Q Plot")
qqline(AutoBi$TARGET, col='red')
```

We'll ignore the case number variable and the loss and claim age variables before transformation. We'll plot the histograms and boxplots of the remaining variables to inspect their distributions.

```{r hists}
# Select columns for use and show summary
data <- AutoBi[, -c(1, 7, 8)]
summary(data)

# Plot histograms
par(mfrow=c(2, 4))
for (i in c(1:length(data))){
  hist(data[, i], xlab=paste(names(data)[i]), main="")
}
```

```{r boxplots}
# Plot boxplots
par(mfrow=c(2, 4))
for (i in c(1:length(data))){
  boxplot(data[, i], xlab=paste(names(data)[i]), main="")
}
```

```{r ggplot}
# Show example of advanced viz w/ggplot
library(ggplot2)
p <- ggplot(data=data, aes(x=NORM_CLMAGE, y=TARGET, group=CLMSEX)) + 
  geom_line(aes(color=factor(CLMSEX))) + 
  geom_smooth(aes(color=factor(CLMSEX)), method="loess") +
  scale_colour_discrete(name="Gender", labels=c("Male", "Female")) + 
  ggtitle("Example GGPlot Visualization") +
  theme(plot.title = element_text(size=14, face="bold"))
p
```

## Statistical Analysis
Before moving on, it's important to examine the correlations, variances, and covariances between the variables to understand their relationships with each other.

```{r stats}
cor(data)
var(data)
cov(data)

# Show scatterplot matrix
pairs(data)
```

We'll also use the VIF test to detect if any multicollinearity is present. We can conclude multicollinearity is not a major concern as the maximum VIF values are less than 10. Finally, we'll save the dataset to a CSV file for later use.

```{r vif}
library(VIF)
vifs <- vif(data$TARGET, data[1:6])
vif_df <- as.data.frame(vifs$modelmatrix)
names(vif_df) <- vifs$select
cat("Max VIF Values:", sapply(vif_df, max))

# Save data to CSV
write.csv(data, file="ABI_data.csv", row.names=FALSE)
```

## Sources
[1] http://www.naic.org/documents/committees_c_catf_related_price_optimization_white_paper.pdf

[2] https://insidebigdata.com/2014/11/09/ask-data-scientist-importance-exploratory-data-analysis/

[3] https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf